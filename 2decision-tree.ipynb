{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created on Oct 12, 2010\n",
      "Update on 2017-05-18\n",
      "Decision Tree Source Code for Machine Learning in Action Ch. 3\n",
      "Author: Peter Harrington/片刻\n",
      "GitHub: https://github.com/apachecn/AiLearning\n",
      "\n",
      "infoGain= 0.03939650364612124 bestFeature= 0 1.3260875253642983 1.286691021718177\n",
      "infoGain= 0.039510835423565815 bestFeature= 1 1.3260875253642983 1.2865766899407325\n",
      "infoGain= 0.37700523001147723 bestFeature= 2 1.3260875253642983 0.9490822953528211\n",
      "infoGain= 0.5487949406953986 bestFeature= 3 1.3260875253642983 0.7772925846688997\n",
      "infoGain= 0.22125183600446618 bestFeature= 0 1.5545851693377994 1.3333333333333333\n",
      "infoGain= 0.09543725231055489 bestFeature= 1 1.5545851693377994 1.4591479170272446\n",
      "infoGain= 0.7704260414863776 bestFeature= 2 1.5545851693377994 0.7841591278514218\n",
      "infoGain= 0.3166890883150208 bestFeature= 0 0.6500224216483541 0.3333333333333333\n",
      "infoGain= 0.19087450462110933 bestFeature= 1 0.6500224216483541 0.4591479170272448\n",
      "infoGain= 1.0 bestFeature= 0 1.0 0.0\n",
      "infoGain= 0.2516291673878229 bestFeature= 0 0.9182958340544896 0.6666666666666666\n",
      "infoGain= 0.4591479170272448 bestFeature= 1 0.9182958340544896 0.4591479170272448\n",
      "infoGain= 0.9182958340544896 bestFeature= 0 0.9182958340544896 0.0\n",
      "{'tearRate': {'normal': {'astigmatic': {'no': {'age': {'presbyopic': {'prescript': {'myope': 'no lenses', 'hyper': 'soft'}}, 'pre': 'soft', 'young': 'soft'}}, 'yes': {'prescript': {'myope': 'hard', 'hyper': {'age': {'presbyopic': 'no lenses', 'pre': 'no lenses', 'young': 'hard'}}}}}}, 'reduced': 'no lenses'}}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'createPlot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b38166a4ddb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;31m#fishTest()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mContactLensesTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-b38166a4ddb5>\u001b[0m in \u001b[0;36mContactLensesTest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlensesTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;31m# 画图可视化展现\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m     \u001b[0mdtPlot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreatePlot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlensesTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'createPlot'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# coding:utf-8\n",
    "'''\n",
    "Created on Oct 12, 2010\n",
    "Update on 2017-05-18\n",
    "Decision Tree Source Code for Machine Learning in Action Ch. 3\n",
    "Author: Peter Harrington/片刻\n",
    "GitHub: https://github.com/apachecn/AiLearning\n",
    "'''\n",
    "from __future__ import print_function\n",
    "print(__doc__)\n",
    "import operator\n",
    "from math import log\n",
    "#import decisionTreePlot as dtPlot\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as dtPlot\n",
    "import matplotlib as mpl\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def createDataSet():\n",
    "    \"\"\"DateSet 基础数据集\n",
    "    Args:\n",
    "        无需传入参数\n",
    "    Returns:\n",
    "        返回数据集和对应的label标签\n",
    "    \"\"\"\n",
    "    dataSet = [[1, 1, 'yes'],\n",
    "               [1, 1, 'yes'],\n",
    "               [1, 0, 'no'],\n",
    "               [0, 1, 'no'],\n",
    "               [0, 1, 'no']]\n",
    "    # dataSet = [['yes'],\n",
    "    #         ['yes'],\n",
    "    #         ['no'],\n",
    "    #         ['no'],\n",
    "    #         ['no']]\n",
    "    # labels  露出水面   脚蹼\n",
    "    labels = ['no surfacing', 'flippers']\n",
    "    # change to discrete values\n",
    "    return dataSet, labels\n",
    "\n",
    "\n",
    "def calcShannonEnt(dataSet):\n",
    "    \"\"\"calcShannonEnt(calculate Shannon entropy 计算给定数据集的香农熵)\n",
    "    Args:\n",
    "        dataSet 数据集\n",
    "    Returns:\n",
    "        返回 每一组feature下的某个分类下，香农熵的信息期望\n",
    "    \"\"\"\n",
    "    # -----------计算香农熵的第一种实现方式start--------------------------------------------------------------------------------\n",
    "    # 求list的长度，表示计算参与训练的数据量\n",
    "    numEntries = len(dataSet)\n",
    "    # 下面输出我们测试的数据集的一些信息\n",
    "    # 例如：<type 'list'> numEntries:  5 是下面的代码的输出\n",
    "    # print type(dataSet), 'numEntries: ', numEntries\n",
    "\n",
    "    # 计算分类标签label出现的次数\n",
    "    labelCounts = {}\n",
    "    # the the number of unique elements and their occurance\n",
    "    for featVec in dataSet:\n",
    "        # 将当前实例的标签存储，即每一行数据的最后一个数据代表的是标签\n",
    "        currentLabel = featVec[-1]\n",
    "        # 为所有可能的分类创建字典，如果当前的键值不存在，则扩展字典并将当前键值加入字典。每个键值都记录了当前类别出现的次数。\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1\n",
    "        # print '-----', featVec, labelCounts\n",
    "\n",
    "    # 对于label标签的占比，求出label标签的香农熵\n",
    "    shannonEnt = 0.0\n",
    "    for key in labelCounts:\n",
    "        # 使用所有类标签的发生频率计算类别出现的概率。\n",
    "        prob = float(labelCounts[key])/numEntries\n",
    "        # log base 2 \n",
    "        # 计算香农熵，以 2 为底求对数\n",
    "        shannonEnt -= prob * log(prob, 2)\n",
    "        # print '---', prob, prob * log(prob, 2), shannonEnt\n",
    "    # -----------计算香农熵的第一种实现方式end--------------------------------------------------------------------------------\n",
    "\n",
    "    # # -----------计算香农熵的第二种实现方式start--------------------------------------------------------------------------------\n",
    "    # # 统计标签出现的次数\n",
    "    # label_count = Counter(data[-1] for data in dataSet)\n",
    "    # # 计算概率\n",
    "    # probs = [p[1] / len(dataSet) for p in label_count.items()]\n",
    "    # # 计算香农熵\n",
    "    # shannonEnt = sum([-p * log(p, 2) for p in probs])\n",
    "    # # -----------计算香农熵的第二种实现方式end--------------------------------------------------------------------------------\n",
    "    return shannonEnt\n",
    "\n",
    "\n",
    "def splitDataSet(dataSet, index, value):\n",
    "    \"\"\"splitDataSet(通过遍历dataSet数据集，求出index对应的colnum列的值为value的行)\n",
    "        就是依据index列进行分类，如果index列的数据等于 value的时候，就要将 index 划分到我们创建的新的数据集中\n",
    "    Args:\n",
    "        dataSet 数据集                 待划分的数据集\n",
    "        index 表示每一行的index列        划分数据集的特征\n",
    "        value 表示index列对应的value值   需要返回的特征的值。\n",
    "    Returns:\n",
    "        index列为value的数据集【该数据集需要排除index列】\n",
    "    \"\"\"\n",
    "    # -----------切分数据集的第一种方式 start------------------------------------\n",
    "    retDataSet = []\n",
    "    for featVec in dataSet: \n",
    "        # index列为value的数据集【该数据集需要排除index列】\n",
    "        # 判断index列的值是否为value\n",
    "        if featVec[index] == value:\n",
    "            # chop out index used for splitting\n",
    "            # [:index]表示前index行，即若 index 为2，就是取 featVec 的前 index 行\n",
    "            reducedFeatVec = featVec[:index]\n",
    "            '''\n",
    "            请百度查询一下： extend和append的区别\n",
    "            list.append(object) 向列表中添加一个对象object\n",
    "            list.extend(sequence) 把一个序列seq的内容添加到列表中\n",
    "            1、使用append的时候，是将new_media看作一个对象，整体打包添加到music_media对象中。\n",
    "            2、使用extend的时候，是将new_media看作一个序列，将这个序列和music_media序列合并，并放在其后面。\n",
    "            result = []\n",
    "            result.extend([1,2,3])\n",
    "            print result\n",
    "            result.append([4,5,6])\n",
    "            print result\n",
    "            result.extend([7,8,9])\n",
    "            print result\n",
    "            结果：\n",
    "            [1, 2, 3]\n",
    "            [1, 2, 3, [4, 5, 6]]\n",
    "            [1, 2, 3, [4, 5, 6], 7, 8, 9]\n",
    "            '''\n",
    "            reducedFeatVec.extend(featVec[index+1:])\n",
    "            # [index+1:]表示从跳过 index 的 index+1行，取接下来的数据\n",
    "            # 收集结果值 index列为value的行【该行需要排除index列】\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    # -----------切分数据集的第一种方式 end------------------------------------\n",
    "\n",
    "    # # -----------切分数据集的第二种方式 start------------------------------------\n",
    "    # retDataSet = [data for data in dataSet for i, v in enumerate(data) if i == axis and v == value]\n",
    "    # # -----------切分数据集的第二种方式 end------------------------------------\n",
    "    return retDataSet\n",
    "\n",
    "\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    \"\"\"chooseBestFeatureToSplit(选择最好的特征)\n",
    "    Args:\n",
    "        dataSet 数据集\n",
    "    Returns:\n",
    "        bestFeature 最优的特征列\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------选择最优特征的第一种方式 start------------------------------------\n",
    "    # 求第一行有多少列的 Feature, 最后一列是label列嘛\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    # label的信息熵\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    # 最优的信息增益值, 和最优的Featurn编号\n",
    "    bestInfoGain, bestFeature = 0.0, -1\n",
    "    # iterate over all the features\n",
    "    for i in range(numFeatures):\n",
    "        # create a list of all the examples of this feature\n",
    "        # 获取每一个实例的第i+1个feature，组成list集合\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        # get a set of unique values\n",
    "        # 获取剔重后的集合，使用set对list数据进行去重\n",
    "        uniqueVals = set(featList)\n",
    "        # 创建一个临时的信息熵\n",
    "        newEntropy = 0.0\n",
    "        # 遍历某一列的value集合，计算该列的信息熵 \n",
    "        # 遍历当前特征中的所有唯一属性值，对每个唯一属性值划分一次数据集，计算数据集的新熵值，并对所有唯一特征值得到的熵求和。\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            prob = len(subDataSet)/float(len(dataSet))\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)\n",
    "        # gain[信息增益]: 划分数据集前后的信息变化， 获取信息熵最大的值\n",
    "        # 信息增益是熵的减少或者是数据无序度的减少。最后，比较所有特征中的信息增益，返回最好特征划分的索引值。\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        print('infoGain=', infoGain, 'bestFeature=', i, baseEntropy, newEntropy)\n",
    "        if (infoGain > bestInfoGain):\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature\n",
    "    # -----------选择最优特征的第一种方式 end------------------------------------\n",
    "\n",
    "    # # -----------选择最优特征的第二种方式 start------------------------------------\n",
    "    # # 计算初始香农熵\n",
    "    # base_entropy = calcShannonEnt(dataSet)\n",
    "    # best_info_gain = 0\n",
    "    # best_feature = -1\n",
    "    # # 遍历每一个特征\n",
    "    # for i in range(len(dataSet[0]) - 1):\n",
    "    #     # 对当前特征进行统计\n",
    "    #     feature_count = Counter([data[i] for data in dataSet])\n",
    "    #     # 计算分割后的香农熵\n",
    "    #     new_entropy = sum(feature[1] / float(len(dataSet)) * calcShannonEnt(splitDataSet(dataSet, i, feature[0])) \\\n",
    "    #                    for feature in feature_count.items())\n",
    "    #     # 更新值\n",
    "    #     info_gain = base_entropy - new_entropy\n",
    "    #     print('No. {0} feature info gain is {1:.3f}'.format(i, info_gain))\n",
    "    #     if info_gain > best_info_gain:\n",
    "    #         best_info_gain = info_gain\n",
    "    #         best_feature = i\n",
    "    # return best_feature\n",
    "    # # -----------选择最优特征的第二种方式 end------------------------------------\n",
    "\n",
    "\n",
    "def majorityCnt(classList):\n",
    "    \"\"\"majorityCnt(选择出现次数最多的一个结果)\n",
    "    Args:\n",
    "        classList label列的集合\n",
    "    Returns:\n",
    "        bestFeature 最优的特征列\n",
    "    \"\"\"\n",
    "    # -----------majorityCnt的第一种方式 start------------------------------------\n",
    "    classCount = {}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote] = 0\n",
    "        classCount[vote] += 1\n",
    "    # 倒叙排列classCount得到一个字典集合，然后取出第一个就是结果（yes/no），即出现次数最多的结果\n",
    "    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    # print 'sortedClassCount:', sortedClassCount\n",
    "    return sortedClassCount[0][0]\n",
    "    # -----------majorityCnt的第一种方式 end------------------------------------\n",
    "\n",
    "    # # -----------majorityCnt的第二种方式 start------------------------------------\n",
    "    # major_label = Counter(classList).most_common(1)[0]\n",
    "    # return major_label\n",
    "    # # -----------majorityCnt的第二种方式 end------------------------------------\n",
    "\n",
    "\n",
    "def createTree(dataSet, labels):\n",
    "    classList = [example[-1] for example in dataSet]\n",
    "    # 如果数据集的最后一列的第一个值出现的次数=整个集合的数量，也就说只有一个类别，就只直接返回结果就行\n",
    "    # 第一个停止条件：所有的类标签完全相同，则直接返回该类标签。\n",
    "    # count() 函数是统计括号中的值在list中出现的次数\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    # 如果数据集只有1列，那么最初出现label次数最多的一类，作为结果\n",
    "    # 第二个停止条件：使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组。\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "\n",
    "    # 选择最优的列，得到最优列对应的label含义\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)\n",
    "    # 获取label的名称\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    # 初始化myTree\n",
    "    myTree = {bestFeatLabel: {}}\n",
    "    # 注：labels列表是可变对象，在PYTHON函数中作为参数时传址引用，能够被全局修改\n",
    "    # 所以这行代码导致函数外的同名变量被删除了元素，造成例句无法执行，提示'no surfacing' is not in list\n",
    "    del(labels[bestFeat])\n",
    "    # 取出最优列，然后它的branch做分类\n",
    "    featValues = [example[bestFeat] for example in dataSet]\n",
    "    uniqueVals = set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        # 求出剩余的标签label\n",
    "        subLabels = labels[:]\n",
    "        # 遍历当前选择特征包含的所有属性值，在每个数据集划分上递归调用函数createTree()\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)\n",
    "        # print 'myTree', value, myTree\n",
    "    return myTree\n",
    "\n",
    "\n",
    "def classify(inputTree, featLabels, testVec):\n",
    "    \"\"\"classify(给输入的节点，进行分类)\n",
    "    Args:\n",
    "        inputTree  决策树模型\n",
    "        featLabels Feature标签对应的名称\n",
    "        testVec    测试输入的数据\n",
    "    Returns:\n",
    "        classLabel 分类的结果值，需要映射label才能知道名称\n",
    "    \"\"\"\n",
    "    # 获取tree的根节点对于的key值\n",
    "    firstStr = inputTree.keys()[0]\n",
    "    # 通过key得到根节点对应的value\n",
    "    secondDict = inputTree[firstStr]\n",
    "    # 判断根节点名称获取根节点在label中的先后顺序，这样就知道输入的testVec怎么开始对照树来做分类\n",
    "    featIndex = featLabels.index(firstStr)\n",
    "    # 测试数据，找到根节点对应的label位置，也就知道从输入的数据的第几位来开始分类\n",
    "    key = testVec[featIndex]\n",
    "    valueOfFeat = secondDict[key]\n",
    "    print('+++', firstStr, 'xxx', secondDict, '---', key, '>>>', valueOfFeat)\n",
    "    # 判断分枝是否结束: 判断valueOfFeat是否是dict类型\n",
    "    if isinstance(valueOfFeat, dict):\n",
    "        classLabel = classify(valueOfFeat, featLabels, testVec)\n",
    "    else:\n",
    "        classLabel = valueOfFeat\n",
    "    return classLabel\n",
    "\n",
    "\n",
    "def storeTree(inputTree, filename):\n",
    "    import pickle\n",
    "    # -------------- 第一种方法 start --------------\n",
    "    fw = open(filename, 'wb')\n",
    "    pickle.dump(inputTree, fw)\n",
    "    fw.close()\n",
    "    # -------------- 第一种方法 end --------------\n",
    "\n",
    "    # -------------- 第二种方法 start --------------\n",
    "    with open(filename, 'wb') as fw:\n",
    "        pickle.dump(inputTree, fw)\n",
    "    # -------------- 第二种方法 start --------------\n",
    "\n",
    "\n",
    "def grabTree(filename):\n",
    "    import pickle\n",
    "    fr = open(filename,'rb')\n",
    "    return pickle.load(fr)\n",
    "\n",
    "\n",
    "def fishTest():\n",
    "    # 1.创建数据和结果标签\n",
    "    myDat, labels = createDataSet()\n",
    "    # print myDat, labels\n",
    "\n",
    "    # 计算label分类标签的香农熵\n",
    "    # calcShannonEnt(myDat)\n",
    "\n",
    "    # # 求第0列 为 1/0的列的数据集【排除第0列】\n",
    "    # print '1---', splitDataSet(myDat, 0, 1)\n",
    "    # print '0---', splitDataSet(myDat, 0, 0)\n",
    "\n",
    "    # # 计算最好的信息增益的列\n",
    "    # print chooseBestFeatureToSplit(myDat)\n",
    "\n",
    "    import copy\n",
    "    myTree = createTree(myDat, copy.deepcopy(labels))\n",
    "    print(myTree)\n",
    "    # [1, 1]表示要取的分支上的节点位置，对应的结果值\n",
    "    print(classify(myTree, labels, [1, 1]))\n",
    "    \n",
    "    # 获得树的高度\n",
    "    print(get_tree_height(myTree))\n",
    "\n",
    "    # 画图可视化展现\n",
    "    dtPlot.createPlot(myTree)\n",
    "\n",
    "\n",
    "def ContactLensesTest():\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        预测隐形眼镜的测试代码\n",
    "    Args:\n",
    "        none\n",
    "    Returns:\n",
    "        none\n",
    "    \"\"\"\n",
    "\n",
    "    # 加载隐形眼镜相关的 文本文件 数据\n",
    "    fr = open('./lenses.txt')\n",
    "    # 解析数据，获得 features 数据\n",
    "    lenses = [inst.strip().split('\\t') for inst in fr.readlines()]\n",
    "    # 得到数据的对应的 Labels\n",
    "    lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']\n",
    "    # 使用上面的创建决策树的代码，构造预测隐形眼镜的决策树\n",
    "    lensesTree = createTree(lenses, lensesLabels)\n",
    "    print(lensesTree)\n",
    "    # 画图可视化展现\n",
    "    dtPlot.createPlot(lensesTree)\n",
    "    \n",
    "    \n",
    "def get_tree_height(tree):\n",
    "    \"\"\"\n",
    "     Desc:\n",
    "        递归获得决策树的高度\n",
    "    Args:\n",
    "        tree\n",
    "    Returns:\n",
    "        树高\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(tree, dict):\n",
    "        return 1\n",
    "\n",
    "    child_trees = tree.values()[0].values()\n",
    "\n",
    "    # 遍历子树, 获得子树的最大高度\n",
    "    max_height = 0\n",
    "    for child_tree in child_trees:\n",
    "        child_tree_height = get_tree_height(child_tree)\n",
    "\n",
    "        if child_tree_height > max_height:\n",
    "            max_height = child_tree_height\n",
    "\n",
    "    return max_height + 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #fishTest()\n",
    "    ContactLensesTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
